{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-03T15:16:01.177935Z",
     "iopub.status.busy": "2024-05-03T15:16:01.177555Z",
     "iopub.status.idle": "2024-05-03T15:16:01.182608Z",
     "shell.execute_reply": "2024-05-03T15:16:01.181645Z",
     "shell.execute_reply.started": "2024-05-03T15:16:01.177909Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    FOR_SUBMISSION = True\n",
    "else:\n",
    "    FOR_SUBMISSION = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-03T15:16:20.954612Z",
     "iopub.status.busy": "2024-05-03T15:16:20.953972Z",
     "iopub.status.idle": "2024-05-03T15:16:56.920228Z",
     "shell.execute_reply": "2024-05-03T15:16:56.919201Z",
     "shell.execute_reply.started": "2024-05-03T15:16:20.954570Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -U /kaggle/input/bitsandbytes-0-42-0-py3-none-any-whl/bitsandbytes-0.42.0-py3-none-any.whl -qq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-03T15:21:09.270002Z",
     "iopub.status.busy": "2024-05-03T15:21:09.269258Z",
     "iopub.status.idle": "2024-05-03T15:21:09.626091Z",
     "shell.execute_reply": "2024-05-03T15:21:09.625123Z",
     "shell.execute_reply.started": "2024-05-03T15:21:09.269967Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-03T15:21:11.803236Z",
     "iopub.status.busy": "2024-05-03T15:21:11.802767Z",
     "iopub.status.idle": "2024-05-03T15:21:11.807680Z",
     "shell.execute_reply": "2024-05-03T15:21:11.806635Z",
     "shell.execute_reply.started": "2024-05-03T15:21:11.803208Z"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-03T15:21:14.387528Z",
     "iopub.status.busy": "2024-05-03T15:21:14.386841Z",
     "iopub.status.idle": "2024-05-03T15:21:17.616876Z",
     "shell.execute_reply": "2024-05-03T15:21:17.615809Z",
     "shell.execute_reply.started": "2024-05-03T15:21:14.387498Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-03T15:21:20.961613Z",
     "iopub.status.busy": "2024-05-03T15:21:20.961122Z",
     "iopub.status.idle": "2024-05-03T15:23:34.729961Z",
     "shell.execute_reply": "2024-05-03T15:23:34.729214Z",
     "shell.execute_reply.started": "2024-05-03T15:21:20.961565Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7abbd7e9e39a47f08427ba5a54c47b17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "\n",
    "model_name = \"/kaggle/input/deepseek-math-7b-rl/transformers/7b-rl/1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "model.generation_config = GenerationConfig.from_pretrained(model_name)\n",
    "model.generation_config.pad_token_id = model.generation_config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-03T15:25:06.998588Z",
     "iopub.status.busy": "2024-05-03T15:25:06.997744Z",
     "iopub.status.idle": "2024-05-03T15:25:07.005114Z",
     "shell.execute_reply": "2024-05-03T15:25:07.004106Z",
     "shell.execute_reply.started": "2024-05-03T15:25:06.998556Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n"
     ]
    }
   ],
   "source": [
    "def naive_parse(answer):\n",
    "    out = []\n",
    "    start = False\n",
    "    end = False\n",
    "    for l in reversed(list(answer)):\n",
    "        if l in '0123456789' and not end:\n",
    "            start = True\n",
    "            out.append(l)\n",
    "        else:\n",
    "            if start:\n",
    "                end = True\n",
    "        \n",
    "    out = reversed(out)\n",
    "    return ''.join(out)\n",
    "\n",
    "re = naive_parse(\"There are 5 such numbers. So there are 5 special numbers with at most 36 digits. The answer is $\\boxed{52}$.\")\n",
    "print(re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-03T15:25:10.824572Z",
     "iopub.status.busy": "2024-05-03T15:25:10.823967Z",
     "iopub.status.idle": "2024-05-03T15:25:10.836364Z",
     "shell.execute_reply": "2024-05-03T15:25:10.834804Z",
     "shell.execute_reply.started": "2024-05-03T15:25:10.824523Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "\n",
    "def process_output(output):\n",
    "    result = output\n",
    "    \n",
    "    try:\n",
    "        code = output.split('```')[1][7:]\n",
    "\n",
    "        with open('code.py', 'w') as fout:\n",
    "            fout.write(code)\n",
    "\n",
    "        batcmd = 'timeout 7 ' + sys.executable + ' code.py'\n",
    "        try:\n",
    "            shell_output = subprocess.check_output(batcmd, shell=True).decode('utf8')\n",
    "            print(shell_output)\n",
    "            code_output = round(float(eval(shell_output))) % 1000\n",
    "        except:\n",
    "            code_output = -1\n",
    "\n",
    "        print('CODE RESULTS', code_output)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('ERROR PARSING')\n",
    "        code_output = -1\n",
    "    \n",
    "    try:\n",
    "        result_output = re.findall(r'\\\\boxed\\{(.*)\\}', result)\n",
    "\n",
    "        print('BOXED', result_output)\n",
    "        if not len(result_output):\n",
    "            result_output = naive_parse(result)\n",
    "        else:\n",
    "            result_output = result_output[-1]\n",
    "\n",
    "        print('BOXED', result_output)\n",
    "        if not len(result_output):\n",
    "            result_output = -1\n",
    "        \n",
    "        else:\n",
    "            result_output = round(float(eval(result_output))) % 1000\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('ERROR PARSING')\n",
    "        result_output = -1\n",
    "    \n",
    "    return result_output, code_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-03T15:25:17.553388Z",
     "iopub.status.busy": "2024-05-03T15:25:17.552406Z",
     "iopub.status.idle": "2024-05-03T15:25:17.568982Z",
     "shell.execute_reply": "2024-05-03T15:25:17.568177Z",
     "shell.execute_reply.started": "2024-05-03T15:25:17.553352Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not submission\n"
     ]
    }
   ],
   "source": [
    "if not FOR_SUBMISSION:\n",
    "    print(\"not submission\")\n",
    "    df_test = pd.read_csv('/kaggle/input/ai-mathematical-olympiad-prize/train.csv')\n",
    "else:\n",
    "    print(\"submission\")\n",
    "    import sys\n",
    "\n",
    "    sys.path.append(\"/kaggle/input/ai-mathematical-olympiad-prize/\")\n",
    "\n",
    "    import aimo\n",
    "\n",
    "    env = aimo.make_env()\n",
    "    df_test_iter = env.iter_test()\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-03T15:27:17.858768Z",
     "iopub.status.busy": "2024-05-03T15:27:17.858018Z",
     "iopub.status.idle": "2024-05-03T15:33:15.490035Z",
     "shell.execute_reply": "2024-05-03T15:33:15.488640Z",
     "shell.execute_reply.started": "2024-05-03T15:27:17.858736Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "problem\n",
      "0\n",
      "Let $k, l > 0$ be parameters. The parabola $y = kx^2 - 2kx + l$ intersects the line $y = 4$ at two points $A$ and $B$. These points are distance 6 apart. What is the sum of the squares of the distances from $A$ and $B$ to the origin?\n",
      " The parabola $y = kx^2 - 2kx + l$ intersects the line $y = 4$ at two points $A$ and $B$. This means that the solutions to the equation $kx^2 - 2kx + l - 4 = 0$ are the $x$-coordinates of $A$ and $B$. Let's call these solutions $x_1$ and $x_2$. We are given that $|x_2 - x_1| = 6$, which means the distance between $A$ and $B$ is 6.\n",
      "\n",
      "The sum of the squares of the distances from $A$ and $B$ to the origin is $x_1^2 + y_1^2 + x_2^2 + y_2^2$, where $y_1 = 4$ and $y_2 = 4$ because both points lie on the line $y = 4$. So we want to find $x_1^2 + y_1^2 + x_2^2 + y_2^2 = x_1^2 + 16 + x_2^2 + 16 = x_1^2 + x_2^2 + 32$.\n",
      "\n",
      "We know that $x_1$ and $x_2$ are the solutions to the equation $kx^2 - 2kx + l - 4 = 0$. By Vieta's formulas, we have $x_1 + x_2 = \\frac{2k}{k} = 2$ and $x_1x_2 = \\frac{l - 4}{k}$.\n",
      "\n",
      "We also know that $(x_2 - x_1)^2 = (x_2 + x_1)^2 - 4x_1x_2 = 6^2 = 36$. Substituting the values we found for $x_1 + x_2$ and $x_1x_2$, we get $36 = 4 - \\frac{4(l - 4)}{k}$. Simplifying, we have $36k = 4k - 4(l - 4)$, or $32k = -4(l - 4)$. Dividing by 4, we get $8k = -(l - 4)$.\n",
      "\n",
      "We want to find $x_1^2 + x_2^2 + 32$. We know that $(x_1^2 + x_2^2) = (x_1 + x_2)^2 - 2x_1x_2 = 2^2 - 2\\frac{l - 4}{k} = 4 - \\frac{2(l - 4)}{k}$.\n",
      "\n",
      "Substituting $8k = -(l - 4)$, we have $l = 4 - 8k$. So $(x_1^2 + x_2^2) = 4 - \\frac{2(4 - 8k - 4)}{k} = 4 - \\frac{-16k}{k} = 4 + 16 = 20$.\n",
      "\n",
      "Therefore, the sum of the squares of the distances from $A$ and $B$ to the origin is $x_1^2 + y_1^2 + x_2^2 + y_2^2 = x_1^2 + x_2^2 + 32 = 20 + 32 = 52$. The answer is: $52$\n",
      "answer 52\n",
      "answer2 52\n",
      "split_with_sizes(): argument 'split_sizes' (position 2) must be tuple of ints, not str\n",
      "ERROR PARSING\n",
      "expected string or bytes-like object\n",
      "ERROR PARSING\n",
      "res -1\n",
      "code -1\n",
      "problem\n",
      "1\n",
      "Each of the three-digits numbers $111$ to $999$ is coloured blue or yellow in such a way that the sum of any two (not necessarily different) yellow numbers is equal to a blue number. What is the maximum possible number of yellow numbers there can be?\n",
      " Let's consider the number $111$. If it is yellow, then all numbers of the form $111k$ where $k$ is an integer from $1$ to $9$ must be yellow as well, because their sum with $111$ is a blue number. This gives us $9$ yellow numbers.\n",
      "Similarly, if $222$ is yellow, then all numbers of the form $222k$ from $1$ to $9$ must be yellow, giving us $9$ more yellow numbers.\n",
      "Continuing this pattern, we see that if $333, 444, \\ldots, 999$ are yellow, then we get $9$ more yellow numbers each time.\n",
      "Therefore, the maximum number of yellow numbers is achieved when we color all numbers of the form $kkk$ yellow, where $k$ ranges from $1$ to $9$. This gives us a total of $9$ yellow numbers. The answer is: $9$\n",
      "answer 9\n",
      "answer2 9\n",
      "split_with_sizes(): argument 'split_sizes' (position 2) must be tuple of ints, not str\n",
      "ERROR PARSING\n",
      "expected string or bytes-like object\n",
      "ERROR PARSING\n",
      "res -1\n",
      "code -1\n",
      "problem\n",
      "2\n",
      "Let the `sparkle' operation on positive integer $n$ consist of calculating the sum of the digits of $n$ and taking its factorial, e.g. the sparkle of 13 is $4! = 24$. A robot starts with a positive integer on a blackboard, then after each second for the rest of eternity, replaces the number on the board with its sparkle. For some `special' numbers, if they're the first number, then eventually every number that appears will be less than 6. How many such special numbers are there with at most 36 digits?\n",
      " Let's first understand the `sparkle' operation. If we have a number $n$, we first calculate the sum of its digits, let's call this sum $S$. Then we take the factorial of $S$, denoted as $S!$. So the sparkle of $n$ is $S!$.\n",
      "\n",
      "Now, we want to find the special numbers such that if they're the first number on the blackboard, then eventually every number that appears will be less than 6. This means that after applying the `sparkle' operation repeatedly, we will eventually get a number less than 6.\n",
      "\n",
      "The factorial function grows very quickly. The factorial of 5 is 120, which is already greater than 6. The factorial of 4 is 24, which is also greater than 6. The factorial of 3 is 6, which is less than 6. So if we start with a number whose digit sum is 3, then after applying the `sparkle' operation, we will always get a number less than 6.\n",
      "\n",
      "The only single digit numbers whose digit sum is 3 are 3 and 21. But 21 is not a number with at most 36 digits. So the only possible special numbers with at most 36 digits are those whose digit sum is 3.\n",
      "\n",
      "To count the number of such special numbers with at most 36 digits, we need to count the number of positive integers with digit sum 3. The maximum digit sum for a number with at most 36 digits is 9*4=36, so we are looking for numbers with digit sum 3.\n",
      "\n",
      "The smallest number with digit sum 3 is 3. The next number with digit sum 3 is 102. The next number with digit sum 3 is 111. After that, the next number with digit sum 3 is 201. We can continue this pattern to find all the numbers with digit sum 3 and at most 36 digits.\n",
      "\n",
      "The numbers with digit sum 3 and at most 36 digits are: 3, 102, 111, 201, 1002, 1011, 1101, 2001, 10002, 10011, 10101, 11001, 20001, 100002, 100011, 100101, 101001, 110001, 200001, 1000002, 1000011, 1000101, 1001001, 1010001, 1100001, 2000001, 10000002, 10000011, 10000101, 10001001, 10010001, 10100001, 11000001, 20000001.\n",
      "\n",
      "So there are 28 special numbers with at most 36 digits. The answer is: $28$.\n",
      "answer 28\n",
      "answer2 28\n",
      "split_with_sizes(): argument 'split_sizes' (position 2) must be tuple of ints, not str\n",
      "ERROR PARSING\n",
      "expected string or bytes-like object\n",
      "ERROR PARSING\n",
      "res -1\n",
      "code -1\n",
      "problem\n",
      "3\n",
      "What is the minimum value of $5x^2+5y^2-8xy$ when $x$ and $y$ range over all real numbers such that $|x-2y| + |y-2x| = 40$?\n",
      " First, let's rewrite the given equation $|x-2y| + |y-2x| = 40$ as $|2y-x| + |2x-y| = 40$.\n",
      "By the triangle inequality, we know that $|a| + |b| \\ge |a+b|$, so\n",
      "\\[|2y-x| + |2x-y| \\ge |(2y-x) + (2x-y)| = |y| = |y-0|.\\]\n",
      "Therefore, we have $|x-2y| + |y-2x| \\ge |y|$.\n",
      "Since $|x-2y| + |y-2x| = 40$, we must have $|y| = 40$.\n",
      "This means $y = 20$ or $y = -20$.\n",
      "\n",
      "Now, let's substitute $y = 20$ into the equation $|x-2y| + |y-2x| = 40$:\n",
      "\\[|x-40| + |20-2x| = 40.\\]\n",
      "By the triangle inequality again, we have\n",
      "\\[|x-40| + |20-2x| \\ge |(x-40) + (20-2x)| = |-20| = 20.\\]\n",
      "But we know that $|x-40| + |20-2x| = 40$, so we must have equality.\n",
      "This means that the signs of $x-40$ and $20-2x$ must be the same.\n",
      "If $x-40$ and $20-2x$ are both positive, then $x > 40$ and $20 < x$, which is a contradiction.\n",
      "If $x-40$ and $20-2x$ are both negative, then $x < 40$ and $20 > x$, which gives $x = 20$.\n",
      "So when $y = 20$, we have $x = 20$.\n",
      "\n",
      "Similarly, if we substitute $y = -20$ into the equation $|x-2y| + |y-2x| = 40$, we get\n",
      "\\[|x+40| + |-20-2x| = 40.\\]\n",
      "By the triangle inequality, we have\n",
      "\\[|x+40| + |-20-2x| \\ge |(x+40) + (-20-2x)| = |-20| = 20.\\]\n",
      "But we know that $|x+40| + |-20-2x| = 40$, so we must have equality.\n",
      "This means that the signs of $x+40$ and $-20-2x$ must be the same.\n",
      "If $x+40$ and $-20-2x$ are both positive, then $x > -40$ and $-20 < -x$, which is a contradiction.\n",
      "If $x+40$ and $-20-2x$ are both negative, then $x < -40$ and $-20 > -x$, which gives $x = -20$.\n",
      "So when $y = -20$, we have $x = -20$.\n",
      "\n",
      "Now, let's substitute $x = 20$ and $y = 20$ into the expression $5x^2+5y^2-8xy$:\n",
      "\\[5(20)^2 + 5(20)^2 - 8(20)(20) = 2000 + 2000 - 3200 = 800.\\]\n",
      "And let's substitute $x = -20$ and $y = -20$ into the expression $5x^2+5y^2-8xy$:\n",
      "\\[5(-20)^2 + 5(-20)^2 - 8(-20)(-20) = 2000 + 2000 - 3200 = 800.\\]\n",
      "So in both cases, the value of the expression is 800.\n",
      "\n",
      "Therefore, the minimum value of $5x^2+5y^2-8xy$ when $x$ and $y$ range over all real numbers such that $|x-2y| + |y-2x| = 40$ is 800. The answer is: $800$\n",
      "answer 800\n",
      "answer2 800\n",
      "split_with_sizes(): argument 'split_sizes' (position 2) must be tuple of ints, not str\n",
      "ERROR PARSING\n",
      "expected string or bytes-like object\n",
      "ERROR PARSING\n",
      "res -1\n",
      "code -1\n",
      "problem\n",
      "4\n",
      "There exists a unique increasing geometric sequence of five 2-digit positive integers. What is their sum?\n",
      " Let the five terms of the geometric sequence be $a,$ $ar,$ $ar^2,$ $ar^3,$ and $ar^4,$ where $a$ is the first term and $r$ is the common ratio.\n",
      "Since all the terms are 2-digit positive integers, $a$ must be a 2-digit positive integer.\n",
      "The common ratio $r$ must be greater than 1, because the sequence is increasing.\n",
      "The fifth term $ar^4$ must also be a 2-digit positive integer.\n",
      "The only 2-digit positive integer that is a power of a 2-digit positive integer (greater than 1) is $3^5 = 243$.\n",
      "So, we must have $ar^4 = 243$.\n",
      "Since $a$ is a 2-digit positive integer, the only possibility is $a = 27$ and $r = 3$.\n",
      "Thus, the sequence is $27,$ $81,$ $243,$ $729,$ $2187$.\n",
      "The sum of these five terms is $27 + 81 + 243 + 729 + 2187 = 3279$. The answer is: $3279$\n",
      "answer 3279\n",
      "answer2 279\n",
      "split_with_sizes(): argument 'split_sizes' (position 2) must be tuple of ints, not str\n",
      "ERROR PARSING\n",
      "expected string or bytes-like object\n",
      "ERROR PARSING\n",
      "res -1\n",
      "code -1\n",
      "problem\n",
      "5\n",
      "For how many positive integers $m$ does the equation \\[\\vert \\vert x-1 \\vert -2 \\vert=\\frac{m}{100}\\] have $4$ distinct solutions?\n",
      " The equation $\\vert \\vert x-1 \\vert -2 \\vert=\\frac{m}{100}$ has 4 distinct solutions if and only if $\\frac{m}{100}$ is strictly between 1 and 2. This is because the expression on the left side of the equation, $\\vert \\vert x-1 \\vert -2 \\vert$, can take on any value from 0 to 3, and it takes on the value of 1 or 2 only at 4 distinct points. If $\\frac{m}{100}$ is less than 1, then the equation has no solutions. If $\\frac{m}{100}$ is greater than or equal to 2, then the equation has at most 3 solutions. Therefore, $\\frac{m}{100}$ must be strictly between 1 and 2. This means $1 < \\frac{m}{100} < 2$, or equivalently, $100 < m < 200$. The positive integers $m$ that satisfy this inequality are the integers from 101 to 199, inclusive. To find the number of these integers, we subtract the smallest integer in the range from the largest integer in the range and add 1. So the number of positive integers $m$ for which the equation has 4 distinct solutions is $199 - 101 + 1 = 99$. The answer is: $99$\n",
      "answer 99\n",
      "answer2 99\n",
      "split_with_sizes(): argument 'split_sizes' (position 2) must be tuple of ints, not str\n",
      "ERROR PARSING\n",
      "expected string or bytes-like object\n",
      "ERROR PARSING\n",
      "res -1\n",
      "code -1\n",
      "problem\n",
      "6\n",
      "Suppose that we roll four 6-sided fair dice with faces numbered 1 to~6. Let $a/b$ be the probability that the highest roll is a 5, where $a$ and $b$ are relatively prime positive integers. Find $a + b$.\n",
      " The probability that the highest roll is a 5 is the same as the probability that at least one of the dice shows a 5, and the other three dice show a number from 1 to 4.\n",
      "The probability that a single die shows a number from 1 to 4 is $\\frac{4}{6} = \\frac{2}{3}$.\n",
      "The probability that all four dice show a number from 1 to 4 is $\\left(\\frac{2}{3}\\right)^4 = \\frac{16}{81}$.\n",
      "The probability that at least one die shows a 5 is 1 minus the probability that none of the dice show a 5.\n",
      "The probability that a single die does not show a 5 is $\\frac{1}{6}$.\n",
      "The probability that all four dice do not show a 5 is $\\left(\\frac{1}{6}\\right)^4 = \\frac{1}{1296}$.\n",
      "So the probability that at least one die shows a 5 is $1 - \\frac{1}{1296} = \\frac{1295}{1296}$.\n",
      "Therefore, $a = 1295$ and $b = 1296$, and $a + b = 1295 + 1296 = 2591$. The answer is: $2591$\n",
      "answer 2591\n",
      "answer2 591\n",
      "split_with_sizes(): argument 'split_sizes' (position 2) must be tuple of ints, not str\n",
      "ERROR PARSING\n",
      "expected string or bytes-like object\n",
      "ERROR PARSING\n",
      "res -1\n",
      "code -1\n",
      "problem\n",
      "7\n",
      "The points $\\left(x, y\\right)$ satisfying $((\\vert x + y \\vert - 10)^2 + ( \\vert x - y \\vert - 10)^2)((\\vert x \\vert - 8)^2 + ( \\vert y \\vert - 8)^2) = 0$ enclose a convex polygon. What is the area of this convex polygon?\n",
      " The product of two numbers is zero if and only if at least one of the numbers is zero. Therefore, we have two cases to consider:\n",
      "Case 1: $(\\vert x + y \\vert - 10)^2 + ( \\vert x - y \\vert - 10)^2 = 0$.\n",
      "This means that $\\vert x + y \\vert = 10$ and $\\vert x - y \\vert = 10$.\n",
      "The solutions to these equations are the lines $x + y = 10$, $x + y = -10$, $x - y = 10$, and $x - y = -10$.\n",
      "\n",
      "Case 2: $(\\vert x \\vert - 8)^2 + ( \\vert y \\vert - 8)^2 = 0$.\n",
      "This means that $\\vert x \\vert = 8$ and $\\vert y \\vert = 8$.\n",
      "The solutions to these equations are the lines $x = 8$, $x = -8$, $y = 8$, and $y = -8$.\n",
      "\n",
      "The intersection of these lines form a square with side length 10. The area of this square is $10^2 = 100$. The answer is: $100$\n",
      "answer 100\n",
      "answer2 100\n",
      "split_with_sizes(): argument 'split_sizes' (position 2) must be tuple of ints, not str\n",
      "ERROR PARSING\n",
      "expected string or bytes-like object\n",
      "ERROR PARSING\n",
      "res -1\n",
      "code -1\n",
      "problem\n",
      "8\n",
      "Let $ABCD$ be a unit square. Let $P$ be the point on $AB$ such that $|AP| = 1/{20}$ and let $Q$ be the point on $AD$ such that $|AQ| = 1/{24}$. The lines $DP$ and $BQ$ divide the square into four regions. Find the ratio between the areas of the largest region and the smallest region.\n",
      " First, let's find the areas of the four regions.\n",
      "The area of the entire square is 1.\n",
      "The area of triangle $ABQ$ is $\\frac{1}{2} \\cdot \\frac{1}{24} \\cdot 1 = \\frac{1}{48}$.\n",
      "The area of triangle $DCP$ is $\\frac{1}{2} \\cdot \\frac{1}{20} \\cdot 1 = \\frac{1}{40}$.\n",
      "The area of the region between $DP$ and $BQ$ (including the square) is the area of the square minus the areas of the two triangles, which is $1 - \\frac{1}{48} - \\frac{1}{40} = 1 - \\frac{5}{240} - \\frac{3}{240} = 1 - \\frac{8}{240} = 1 - \\frac{1}{30} = \\frac{29}{30}$.\n",
      "The area of the smallest region (the triangle between $DP$ and $BQ$) is $\\frac{1}{2} \\cdot \\frac{1}{20} \\cdot \\frac{1}{24} = \\frac{1}{480}$.\n",
      "The ratio of the area of the largest region to the smallest region is $\\frac{\\frac{29}{30}}{\\frac{1}{480}} = \\frac{29}{30} \\cdot 480 = 456$. The answer is: $456$\n",
      "answer 456\n",
      "answer2 456\n",
      "split_with_sizes(): argument 'split_sizes' (position 2) must be tuple of ints, not str\n",
      "ERROR PARSING\n",
      "expected string or bytes-like object\n",
      "ERROR PARSING\n",
      "res -1\n",
      "code -1\n",
      "problem\n",
      "9\n",
      "A function $f: \\mathbb N \\to \\mathbb N$ satisfies the following two conditions for all positive integers $n$:$f(f(f(n)))=8n-7$ and $f(2n)=2f(n)+1$. Calculate $f(100)$.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 26.12 MiB is free. Process 3895 has 15.87 GiB memory in use. Of the allocated memory 15.43 GiB is allocated by PyTorch, and 152.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m messages \u001b[38;5;241m=\u001b[39m [ {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: problem \u001b[38;5;241m+\u001b[39m instruction} ]\n\u001b[1;32m     33\u001b[0m input_tensor \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mapply_chat_template(messages, add_generation_prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 34\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4098\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m result \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m][input_tensor\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1527\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massisted_decoding(\n\u001b[1;32m   1510\u001b[0m         input_ids,\n\u001b[1;32m   1511\u001b[0m         candidate_generator\u001b[38;5;241m=\u001b[39mcandidate_generator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1523\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1524\u001b[0m     )\n\u001b[1;32m   1525\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGREEDY_SEARCH:\n\u001b[1;32m   1526\u001b[0m     \u001b[38;5;66;03m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1527\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_greedy_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1528\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1531\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1532\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1533\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1534\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1535\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1536\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1537\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1538\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mCONTRASTIVE_SEARCH:\n\u001b[1;32m   1542\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:2411\u001b[0m, in \u001b[0;36mGenerationMixin._greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2408\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2410\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2411\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2412\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2413\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2414\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2415\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2416\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2418\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2419\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1196\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1193\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1196\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1197\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1201\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1202\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1203\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1204\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1205\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1206\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1207\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1209\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1016\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1005\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1006\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1007\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1013\u001b[0m         cache_position,\n\u001b[1;32m   1014\u001b[0m     )\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1016\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1026\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1028\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:739\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    736\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    738\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 739\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    742\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    744\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    745\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    748\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    749\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    751\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:670\u001b[0m, in \u001b[0;36mLlamaSdpaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position)\u001b[0m\n\u001b[1;32m    667\u001b[0m     key_states \u001b[38;5;241m=\u001b[39m key_states\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    668\u001b[0m     value_states \u001b[38;5;241m=\u001b[39m value_states\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m--> 670\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_dropout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    678\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    679\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mview(bsz, q_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 26.12 MiB is free. Process 3895 has 15.87 GiB memory in use. Of the allocated memory 15.43 GiB is allocated by PyTorch, and 152.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "instruction = '\\nPlease reason step by step, and put your final answer within \\\\boxed{}.'\n",
    "if FOR_SUBMISSION:\n",
    "    i = 0\n",
    "    for test, submission in df_test_iter:\n",
    "        try:     \n",
    "            problem = test[\"problem\"]\n",
    "            print(i)\n",
    "            print(\"problem\")\n",
    "            print(problem)\n",
    "            messages = [ {\"role\": \"user\", \"content\": problem + instruction} ]\n",
    "            input_tensor = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "            outputs = model.generate(input_tensor.to(model.device), max_new_tokens=4098)\n",
    "            result = tokenizer.decode(outputs[0][input_tensor.shape[1]:], skip_special_tokens=True)\n",
    "            print(result)\n",
    "            answer = naive_parse(result)\n",
    "            print('answer', answer)\n",
    "            answer2 = int(answer) % 1000\n",
    "            print('answer2', answer2)\n",
    "            submission['answer'] = answer2\n",
    "            env.predict(submission)\n",
    "        except:\n",
    "            print('except')\n",
    "            print(i)\n",
    "            submission['answer'] = 0\n",
    "            env.predict(submission)\n",
    "else:\n",
    "    for index, row in df_test.iterrows():\n",
    "        problem = row[\"problem\"]\n",
    "        print(\"problem\")\n",
    "        print(index)\n",
    "        print(problem)\n",
    "        messages = [ {\"role\": \"user\", \"content\": problem + instruction} ]\n",
    "        input_tensor = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "        outputs = model.generate(input_tensor.to(model.device), max_new_tokens=4098)\n",
    "\n",
    "        result = tokenizer.decode(outputs[0][input_tensor.shape[1]:], skip_special_tokens=True)\n",
    "        print(result)\n",
    "        answer = naive_parse(result)\n",
    "        print('answer', answer)\n",
    "        answer2 = int(answer) % 1000\n",
    "        print('answer2', answer2)\n",
    "        \n",
    "        res, code = process_output(outputs)\n",
    "        print('res', res)\n",
    "        print('code', code)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 8365361,
     "sourceId": 73231,
     "sourceType": "competition"
    },
    {
     "datasetId": 4281572,
     "sourceId": 7369493,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4918335,
     "sourceId": 8284906,
     "sourceType": "datasetVersion"
    },
    {
     "modelInstanceId": 25261,
     "sourceId": 30046,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
